{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version was created by chat GPT based on my research paper      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_ABBREVIATIONS = {'1A': 'First Amendment to the United States Constitution', '27A': 'Twenty-Seventh Amendment to the United States Constitution', 'A.': 'Atlantic Reporter', 'A.2d': 'Atlantic Reporter, 2nd Series', 'a/a/o': 'as assignee of', 'AAS': 'Acta Apostolicae Sedis', 'ABA': 'American Bar Association', 'AC': 'Appeal Cases', 'ACC': 'Association of Corporate Counsel', 'AD': 'South African Law Reports, Appellate Division', 'ad.': 'at the suit of', 'ads.': 'at the suit of', 'adsm.': 'at the suit of', 'adj.': 'adjoining', \"Aff'd\": 'affirmed', 'AG': 'Attorney General for England and Wales', 'A-G': 'Advocate general', 'Ala. Admin. Code': 'Alabama Administrative Code', 'Ala. Code': 'Code of Alabama 1975', 'Alaska Admin. Code': 'Alaska Administrative Code', 'Alaska Stat.': 'Alaska Statutes', 'All ER': 'All England Law Reports', 'All SA': 'All South African Law Reports', 'A.L.R.': 'American Law Reports', 'A.L.R.2d': 'American Law Reports, 2nd Series', 'A.L.R.3d': 'American Law Reports, 3rd Series', 'A.L.R.4th': 'American Law Reports, 4th Series', 'A.L.R.5th': 'American Law Reports, 5th Series', 'A.L.R.6th': 'American Law Reports, 6th Series', 'A.L.R. Fed.': 'American Law Reports, Federal', 'Am. Jur.': 'American Jurisprudence', 'Am. Jur. 2d.': 'American Jurisprudence, 2nd Series', 'Anor': 'Another', 'Anors': 'Others', 'Ap. const.': 'apostolic constitution', 'Ariz. Admin. Code': 'Arizona Administrative Code', 'Ariz. Admin. Reg.': 'Arizona Administrative Register', 'Ariz. Rev. Stat.': 'Arizona Revised Statutes', 'A.C.A': 'Arkansas Code Annotated', 'Art.': 'Article', 'Artt.': 'Articles', \"Ass'n\": 'Association', 'A.S.S.': 'Acta Sanctae Sedis', 'ATS': 'At the suit of', 'Atty': 'Attorney', 'B.': 'baron', 'B.A.P.': 'Bankruptcy Appellate Panel', 'BCLR': 'Butterworths Constitutional Law Reports (South Africa)', 'BFP': 'Bona fide purchaser', 'Bla.Com.': \"Blackstone's Commentaries on the Laws of England\", 'Bl. Com.': \"Blackstone's Commentaries on the Laws of England\", 'BLLR': 'Butterworths Labour Law Reports (South Africa)', 'b/o': 'on behalf of', 'o/b/o': 'on behalf of', 'BR': 'Bankruptcy', 'B/R': 'Bankruptcy', 'c.': 'chapter', 'cc.': 'chapters', 'CA': 'Class action or Court of Appeal', 'CB': 'Casebook', 'CBJ': 'California Bar journal', 'CC': 'Commerce Clause', 'CCEO': 'Codex Canonum Ecclesiarum Orientalium', 'CCH': 'Commerce Clearing House', 'C-C': 'Counterclaim', 'CE': 'Collateral estoppel', 'CD': 'Closing disclosure', 'CL': 'Common law', 'CNeg': 'Contributory negligence', 'CA #': 'Court of Appeals', 'CA Fed.': 'Court of Appeals for the Federal Circuit[2]', 'Cx': 'Constitution', 'Cx-C': 'Cross-claim', 'Cxl': 'Constitutional', 'Cal. Code': 'California Code', 'Cal. Code Reg.': 'California Code of Regulations', 'CCR': 'California Code of Regulations', 'Cert.': 'Certiorari', 'CIC': 'the Code of Canon Law', 'CIF': 'Coming into force', 'C.F.R.': 'Code of Federal Regulations', 'CFR': 'Code of Federal Regulations', 'CJ': 'Corpus Juris', 'CJEU': 'Court of Justice of the European Union', 'CJS': 'Corpus Juris Secundum', 'CLSA': 'Canon Law Society of America', 'Co. Lit. or Co. Litt.': 'Coke on Littleton', 'Co.': 'company', \"Comm'n\": 'commission', \"Comm'r\": 'commissioner', 'Cong. Rec.': 'Congressional Record', 'Cor.': 'corner', 'Corp.': 'Corporation', 'CRS': 'Congressional Research Service', 'Ct. Cl.': 'the United States Court of Federal Claims Reporter', 'C': 'Contract', 'Δ': 'Defendant', 'D': 'Defendant', 'DAC': 'Days After Contract', 'd/b/a': 'doing business as', 'Decr.': 'Decretum', \"Dep't\": 'department', 'DLR': 'Dominion Law Reports', 'DoCRA': 'Duty of Care Risk Analysis Standard', 'DWOP': 'dismissal for want of prosecution', 'ECHR': 'European Convention on Human Rights', 'ECtHR': 'European Court of Human Rights', 'EComHR': 'European Commission of Human Rights', 'ECJ': 'European Court of Justice', 'EGC': 'European General Court', 'ELR': 'European Law Reporter', 'ER': 'Employer', 'EE': 'Employee', 'Esq.': 'Esquire', 'et als.': 'and others', 'et seq.': 'and following', 'ex. p.': 'Ex parte', 'F.': 'Federal Reporter', 'F.2d': 'Federal Reporter, 2nd Series', 'F.3d': 'Federal Reporter, 3rd Series', \"F.App'x\": 'Federal Appendix', 'F.Cas.': 'Federal Cases 1789:1880', 'Fed. Reg.': 'Federal Register', 'FR': 'Federal Register', 'Fed. R. Bankr. P.': 'Federal Rules of Bankruptcy Procedure', 'Fed. R. Civ. P.': 'Federal Rules of Civil Procedure', 'FRCP': 'Federal Rules of Civil Procedure', 'Fed. R. Crim. P.': 'Federal Rules of Criminal Procedure', 'Fed. R. Evid.': 'Federal Rules of Evidence', 'FRE': 'Federal Rules of Evidence', 'f/k/a': 'formerly known as', 'F. Supp.': 'Federal Supplement', 'F. Supp. 2d': 'Federal Supplement, 2nd Series', 'f/t/a': 'failed to appear', 'GAL': 'Guardian ad litem', 'GATT': 'General Agreement on Tariffs and Trade', 'GC': 'General Counsel', 'GVR': 'Grant, Vacate, and Remand', 'NGO': 'Non Government Organization', \"Gov't\": 'government', 'HC': 'Hypothetical Client or High Court', 'HDC': 'Holder in due course', 'ICJ': 'International Court of Justice', 'Id.': 'the same', 'I.L.M.': 'International Legal Materials', 'ILJ': 'Industrial Law Journal', 'IRB': 'Internal Revenue Bulletin', 'ILRM': 'Irish Law Reports Monthly', 'IR': 'Irish Law Reports', 'IRC': 'Internal Revenue Code', 'ISLN': 'International Standard Lawyer Number', 'Inc.': 'Incorporated', 'Ins.': 'insurance', 'Instr.': 'Instructio, a kind of decree', 'In re': 'in the matter of', \"Int'l\": 'international', 'J.': 'Judge', 'JA': 'Appellate judge', 'JD': 'Juris Doctor', 'JDX': 'jurisdiction', 'JCD': 'Juris Canonici Doctor, Doctor of Canon Law', 'JCL': 'Juris Canonici Licentiatus, Licentiate of Canon Law', 'JOL': 'Judgments Online', 'JJ': 'Judges', 'JMOL': 'Judgment as a matter of law', 'JNOV': 'Judgment notwithstanding verdict', 'Jx': 'Jurisdiction', 'JU': 'disposed of by Judge', 'JUST.': 'Justice', 'KC': \"King's Counsel\", 'L/C': 'Letter of credit', 'L&T': 'Landlord & Tenant', 'L.Ed': 'Lawyers Edition', 'L.Ed.2d': 'Lawyer’s Edition 2d Series', 'LJ': 'Postnominals of a Lord or Lady Justice of Appeal', 'LJJ': 'Postnominals of Lords or Ladies Justice of Appeal', 'LL.B.': 'Bachelor of Laws', 'LLC': 'Limited liability company', 'LL.D.': 'Doctor of Law', 'LL.M.': 'Master of Laws', 'LP': 'Limited partnership', 'LLP': 'Limited liability partnership', 'LLLP': 'Limited liability limited partnership', 'LOI': 'Letter of Intent', 'Ltd.': 'Limited', 'MIL': 'Motion in limine', 'MLR': 'Modern Law Review', 'MOU': 'Memorandum of Understanding', 'M.P.': 'motu proprio', 'MPC': 'Model Penal Code', 'MR': 'Postnominals of the Master of the Rolls', 'MSJ': 'Motion for summary judgment', \"Nat'l\": 'national', 'NDA': 'Non-Disclosure Agreement', 'n/k/a': 'Now Known As', 'N.E.': 'Northeastern Reporter', 'N.E.2d': 'North Eastern Reporter, 2nd Series', 'No.': 'Number', 'N.W.': 'Northwestern Reporter', 'N.W.2d': 'North Western Reporter, 2nd Series', 'oao': 'on the application of', \"Opp'n\": 'opposition', 'O.R.C.': 'Ohio Revised Code', 'Org.': 'organization', 'Ors': 'Others', '¶ (Pilcrow)': 'Paragraph', 'Π (Greek letter Pi)': 'Plaintiff', 'P.': 'Pacific Reporter', 'P.2d': 'Pacific Reporter, 2nd Series', 'P.3d': 'Pacific Reporter, 3rd Series', 'p.': 'Page', 'pp.': 'Pages', 'PA': 'Professional association', 'PC': 'Professional corporation', 'PH': 'Prentice Hall Weekly Legal Service', 'PL': 'Public Law', 'PLLC': 'Professional limited liability company', 'POA': 'power of attorney', 'Prae.': 'Praenotanda', 'Pty': 'proprietary company', 'Pub.L.': 'Public Law', 'QC': \"Queen's Counsel\", 'QDRO': 'Qualified Domestic Relations Order', 'R': 'Rex or Regina', 'RCW': 'Revised Code of Washington', 'R.E.': 'Real Estate', 'R/E': 'Real Estate', 'Re': 'In re', \"Reh'g\": 'Rehearing', 'Relv.': 'Relevant', 'Rescr.': 'Rescriptum', 'Resp.': 'Responsum', \"Resp't\": 'Respondent', \"Rev'd\": 'reversed', 'Rev. Proc.': 'Revenue Procedure', 'Rev. Rul.': 'Revenue Ruling', 'RJ': 'Restorative justice', 'R.O.I': 'Release of Information', 'Canon law': 'Regulæ Juris of Boniface VIII', 'Common law': 'Recurring Judgement', 'R.I.A.A.': 'Reports of International Arbitral Awards', '§': 'Section', 's.': 'Section','§§': 'Sections', 'ss.':'Sections', 'SA': 'South African Law Reports', 'SACR': 'South African Criminal Law Reports', 'SALLR': 'South African Labour Law Reports', 'SC': 'Senior Counsel', 'sc.': 'scilicet', 'sd': 'said', 'S.C.R.': 'Supreme Court Reports', 'SCR': 'Supreme Court Reports', 'S. Ct.': 'Supreme Court Reporter', 'S.E.': 'Southeastern Reporter', 'S.E.2d': 'South Eastern Reporter, 2nd Series', 'SCOTUS': 'Supreme Court of the United States', 'SI': 'Statutory instruments', 'S/J': 'Summary judgment', 'SMJ': 'Subject-matter jurisdiction', 'So.': 'Southern Reporter', 'So. 2d': 'Southern Reporter, 2nd Series', 'SOL': 'Statute of Limitations', 'SOR': 'Statutory Orders and Regulations', 'S.R.R.': 'Tribunal of the Roman Rota', 'SRRDec': 'Sacræ Rotæ Romanæ Decisiones[5]', 'Stat.': 'U.S. Statutes-at-Large', 'S.W.': 'Southwestern Reporter', 'S.W.2d': 'South Western Reporter, 2nd Series', 'S.W.3d': 'South Western Reporter, 3rd Series', 'T.C.': 'United States Tax Court Reports', 'T.D.': 'Treasury Decision', '™': 'Trademark', 'TM': 'Trademark', 'UD': 'Unnatural Death', 'UCC': 'Uniform Commercial Code', 'UCMJ': 'Uniform Code of Military Justice', 'UKPC': 'Privy Council of the United Kingdom', 'UKSC': 'Supreme Court of the United Kingdom', 'UPC': 'Uniform Probate Code', 'USC': 'United States Code', 'USCA': 'United States Code Annotated', 'USCCAN': 'United States Code Congressional and Administrative News', 'USCS': 'United States Code Service', 'UST': 'United States Treaties and Other International Agreements', 'v.': 'versus', 'V-C': 'Postnominals of the Vice-Chancellor of the High Court', 'VC': 'Postnominals of the Vice-Chancellor of the High Court', 'VOP': 'Violation of probation', 'WAC': 'Washington Administrative Code', 'WTO': 'World Trade Organization', 'W. Va. Code': 'West Virginia Code', 'WOP': 'without prejudice', 'w/o/p': 'without prejudice', 'XFD': 'Examination for Discovery', 'XN': 'Examination in Chief', 'XXN': 'Cross-examination', 'B.U. L.': 'Boston University Law Review', 'Buff. L.': 'Buffalo Law Review', 'Cal. L.': 'California Law Review', 'Case W. Res. L.': 'Case Western Reserve Law Review', 'Colum. L.': 'Columbia Law Review', 'Cornell L.': 'Cornell Law Review', 'Duke L.J.': 'Duke Law Journal', 'Fordham L.': 'Fordham Law Review', 'Geo. L.J.': 'Georgetown Law Journal', 'Geo. Wash. L.': 'George Washington Law Review', 'Harv. L.': 'Harvard Law Review', 'How. L.J.': 'Howard Law Journal', 'Mich. L.': 'Michigan Law Review', 'Minn. L.': 'Minnesota Law Review', 'N.Y.U. L.': 'New York University Law Review', 'Ohio St. L.J.': 'Ohio State Law Journal', 'Rutgers L.': 'Rutgers Law Review', 'Seton Hall L.': 'Seton Hall Law Review', 'Stan. L.': 'Stanford Law Review', 'Sup. Ct.': 'Supreme Court Review', 'Temp. L.': 'Temple Law Review', 'Tex. L.': 'Texas Law Review', 'UCLA L.': 'UCLA Law Review', 'U. Chi. L.': 'University of Chicago Law Review', 'U. Pa. L.': 'University of Pennsylvania Law Review', 'Vand. L.': 'Vanderbilt Law Review', 'Va. L.': 'Virginia Law Review', 'Wash. U. L.Q.': 'Washington University Law Quarterly', 'Wis. L.': 'Wisconsin Law Review', 'Yale L.J.': 'Yale Law Journal', 'A.A.': 'Atlantic Reporter', 'ADA': 'Americans with Disabilities Act', 'ALR': 'American Law Reports', 'ALR2d': 'American Law Reports, 2d Series', 'AmJur': 'American Jurisprudence', 'AmJur2d': 'American Jurisprudence 2d', 'AmJurPOF': 'American Jurisprudence Proof of Facts', 'B.R.': 'or B.R.W. Bankruptcy Reporter', 'Cal.': 'Rptr. California Reporter (West’s)', 'CrL': 'Criminal Law Reporter', 'FamLRep.': 'Family Law Reporter (BNA)', 'Fed.Cl.': 'Federal Claims Reporter', 'Fed.R.Evid.Serv.': 'Federal Rules of Evidence Service', 'FEP': 'Cases Fair Employment Practice Cases (BNA)', 'F.R.': 'Federal Register', 'FRD': 'Federal Rules Decisions', 'F.S.': 'or F.Supp. Federal Supplement', 'H.': 'or Haw. Hawai‘i Reports', 'H.A.': 'or Haw.App. Hawai‘i Appellate Reports', 'HBJ': 'Hawai‘i Bar Journal', 'HBN': 'Hawai‘i Bar News', 'HLR': 'Hawai‘i Legal Reporter', 'HRS': 'Hawai‘i Revised Statutes', 'HRSA': 'Hawai‘i Revised Statutes Annotated', 'Last': 'Updated: January', 'Abbreviation': 'Title Class No. or Call No. Range', 'I': '& N Dec. Immigration and Nationality Decisions', 'LRRM': 'Labor Relations Reference Manual (BNA)', 'Moore’s': 'Moore’s Federal Practice', 'N.Y.S.': 'New York Supplement', 'P.L.': 'Public Laws (in U.S. Statutes-at-Large)', 'PUR': 'Public Utilities Reports', 'S.Ct.': 'Supreme Court Reporter', 'T.I.A.S.': 'United States Treaties and other International Act Series', 'UHLR': 'or U.Hawai‘i L.Rev. University of Hawai‘i Law Review', 'ULA': 'Uniform Laws Annotated', 'U.S.C.': 'United States Code', 'U.S.C.A.': 'United States Code Annotated', 'USLW': 'United States Law Week', 'U.S.D.C.': 'Haw Reports of the U.S. District Court of Hawai‘i', 'U.S.T.': 'United States Treaties and Other International Agreements', 'USTC': 'U.S. Tax Cases (CCH)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  21%|██        | 12734/60000 [16:01<59:28, 13.25 examples/s]  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processed_dataset\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Usage example:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m processed_dataset = \u001b[43mpreprocess_lexglue\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mledgar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpreprocess_lexglue\u001b[39m\u001b[34m(dataset_name, model_name)\u001b[39m\n\u001b[32m     60\u001b[39m bert_model = AutoModel.from_pretrained(model_name)\n\u001b[32m     62\u001b[39m map_fn = create_map_function(bert_model, tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m processed_dataset = \u001b[43mfull_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m processed_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/dataset_dict.py:941\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    939\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    962\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:3074\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3069\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3070\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3071\u001b[39m         total=pbar_total,\n\u001b[32m   3072\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3073\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3074\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:3492\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3491\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3492\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3494\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:3466\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3466\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_dataset.py:3389\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3389\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcreate_map_function.<locals>.preprocess_example\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     50\u001b[39m text = remove_metadata(text)\n\u001b[32m     51\u001b[39m paragraphs = segment_paragraphs(text)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m selected = \u001b[43msemantic_filtering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(selected)}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36msemantic_filtering\u001b[39m\u001b[34m(paragraphs, model, tokenizer, top_k)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msemantic_filtering\u001b[39m(paragraphs, model, tokenizer, top_k=\u001b[32m5\u001b[39m):\n\u001b[32m     36\u001b[39m     para_texts = [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paragraphs \u001b[38;5;28;01mif\u001b[39;00m p.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     encoded_paras = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     39\u001b[39m         para_embeds = model(**encoded_paras).last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2975\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m         )\n\u001b[32m   2974\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2998\u001b[39m         text=text,\n\u001b[32m   2999\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3177\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3167\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3168\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3169\u001b[39m     padding=padding,\n\u001b[32m   3170\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3174\u001b[39m     **kwargs,\n\u001b[32m   3175\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3179\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:572\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[32m    571\u001b[39m sanitized_tokens = {}\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_and_encodings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m].keys():\n\u001b[32m    573\u001b[39m     stack = [e \u001b[38;5;28;01mfor\u001b[39;00m item, _ \u001b[38;5;129;01min\u001b[39;00m tokens_and_encodings \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m item[key]]\n\u001b[32m    574\u001b[39m     sanitized_tokens[key] = stack\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# file: legal_classification_pipeline.py\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Step 1: Abbreviation Dictionary (Handcrafted Example)\n",
    "LEGAL_ABBREVIATIONS = {\n",
    "    \"U.S.C.\": \"United States Code\",\n",
    "    \"id.\": \"same as previous citation\",\n",
    "    \"Pet’r\": \"Petitioner\",\n",
    "    \"Ltd.\": \"Limited\"\n",
    "    # Add more abbreviations as needed\n",
    "}\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for abbr, full in LEGAL_ABBREVIATIONS.items():\n",
    "        text = re.sub(rf\"\\\\b{re.escape(abbr)}\\\\b\", full, text)\n",
    "    return text\n",
    "\n",
    "# Step 2: Metadata Removal (Regex based)\n",
    "def remove_metadata(text):\n",
    "    text = re.sub(r\"^.*?Decided on.*?$\", \"\", text, flags=re.MULTILINE)  # Example: remove dates\n",
    "    text = re.sub(r\"^.*?Case No.*?$\", \"\", text, flags=re.MULTILINE)     # Example: remove case numbers\n",
    "    return text\n",
    "\n",
    "# Step 3: Paragraph Segmentation\n",
    "def segment_paragraphs(text):\n",
    "    return [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "\n",
    "# Step 4: Paragraph Filtering using STF-IDF\n",
    "def semantic_filtering(paragraphs, model, tokenizer, top_k=5):\n",
    "    para_texts = [p for p in paragraphs if p.strip()]\n",
    "    encoded_paras = tokenizer(para_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        para_embeds = model(**encoded_paras).last_hidden_state[:, 0, :]\n",
    "\n",
    "    doc_embedding = para_embeds.mean(dim=0, keepdim=True)\n",
    "    similarities = cosine_similarity(para_embeds, doc_embedding)\n",
    "    top_k_indices = similarities[:, 0].argsort()[-top_k:][::-1]\n",
    "    return [para_texts[i] for i in top_k_indices]\n",
    "\n",
    "# Step 5: Map-style Preprocessing Function\n",
    "def create_map_function(model, tokenizer, top_k=5):\n",
    "    def preprocess_example(example):\n",
    "        text = expand_abbreviations(example['text'])\n",
    "        text = remove_metadata(text)\n",
    "        paragraphs = segment_paragraphs(text)\n",
    "        selected = semantic_filtering(paragraphs, model, tokenizer, top_k=top_k)\n",
    "        return {\"text\": \"\\n\".join(selected)}\n",
    "    return preprocess_example\n",
    "\n",
    "# Step 6: Load LexGLUE Dataset and Apply Preprocessing\n",
    "def preprocess_lexglue(dataset_name, model_name=\"nlpaueb/legal-bert-base-uncased\"):\n",
    "    full_dataset = load_dataset(\"lex_glue\", dataset_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    map_fn = create_map_function(bert_model, tokenizer)\n",
    "    processed_dataset = full_dataset.map(map_fn, batched=False)\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "# Usage example:\n",
    "processed_dataset = preprocess_lexglue(\"ledgar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
