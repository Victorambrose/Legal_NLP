{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This version was created by chat GPT based on my research paper      \n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4,5,6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_ABBREVIATIONS = {'1A': 'First Amendment to the United States Constitution', '27A': 'Twenty-Seventh Amendment to the United States Constitution', 'A.': 'Atlantic Reporter', 'A.2d': 'Atlantic Reporter, 2nd Series', 'a/a/o': 'as assignee of', 'AAS': 'Acta Apostolicae Sedis', 'ABA': 'American Bar Association', 'AC': 'Appeal Cases', 'ACC': 'Association of Corporate Counsel', 'AD': 'South African Law Reports, Appellate Division', 'ad.': 'at the suit of', 'ads.': 'at the suit of', 'adsm.': 'at the suit of', 'adj.': 'adjoining', \"Aff'd\": 'affirmed', 'AG': 'Attorney General for England and Wales', 'A-G': 'Advocate general', 'Ala. Admin. Code': 'Alabama Administrative Code', 'Ala. Code': 'Code of Alabama 1975', 'Alaska Admin. Code': 'Alaska Administrative Code', 'Alaska Stat.': 'Alaska Statutes', 'All ER': 'All England Law Reports', 'All SA': 'All South African Law Reports', 'A.L.R.': 'American Law Reports', 'A.L.R.2d': 'American Law Reports, 2nd Series', 'A.L.R.3d': 'American Law Reports, 3rd Series', 'A.L.R.4th': 'American Law Reports, 4th Series', 'A.L.R.5th': 'American Law Reports, 5th Series', 'A.L.R.6th': 'American Law Reports, 6th Series', 'A.L.R. Fed.': 'American Law Reports, Federal', 'Am. Jur.': 'American Jurisprudence', 'Am. Jur. 2d.': 'American Jurisprudence, 2nd Series', 'Anor': 'Another', 'Anors': 'Others', 'Ap. const.': 'apostolic constitution', 'Ariz. Admin. Code': 'Arizona Administrative Code', 'Ariz. Admin. Reg.': 'Arizona Administrative Register', 'Ariz. Rev. Stat.': 'Arizona Revised Statutes', 'A.C.A': 'Arkansas Code Annotated', 'Art.': 'Article', 'Artt.': 'Articles', \"Ass'n\": 'Association', 'A.S.S.': 'Acta Sanctae Sedis', 'ATS': 'At the suit of', 'Atty': 'Attorney', 'B.': 'baron', 'B.A.P.': 'Bankruptcy Appellate Panel', 'BCLR': 'Butterworths Constitutional Law Reports (South Africa)', 'BFP': 'Bona fide purchaser', 'Bla.Com.': \"Blackstone's Commentaries on the Laws of England\", 'Bl. Com.': \"Blackstone's Commentaries on the Laws of England\", 'BLLR': 'Butterworths Labour Law Reports (South Africa)', 'b/o': 'on behalf of', 'o/b/o': 'on behalf of', 'BR': 'Bankruptcy', 'B/R': 'Bankruptcy', 'c.': 'chapter', 'cc.': 'chapters', 'CA': 'Class action or Court of Appeal', 'CB': 'Casebook', 'CBJ': 'California Bar journal', 'CC': 'Commerce Clause', 'CCEO': 'Codex Canonum Ecclesiarum Orientalium', 'CCH': 'Commerce Clearing House', 'C-C': 'Counterclaim', 'CE': 'Collateral estoppel', 'CD': 'Closing disclosure', 'CL': 'Common law', 'CNeg': 'Contributory negligence', 'CA #': 'Court of Appeals', 'CA Fed.': 'Court of Appeals for the Federal Circuit[2]', 'Cx': 'Constitution', 'Cx-C': 'Cross-claim', 'Cxl': 'Constitutional', 'Cal. Code': 'California Code', 'Cal. Code Reg.': 'California Code of Regulations', 'CCR': 'California Code of Regulations', 'Cert.': 'Certiorari', 'CIC': 'the Code of Canon Law', 'CIF': 'Coming into force', 'C.F.R.': 'Code of Federal Regulations', 'CFR': 'Code of Federal Regulations', 'CJ': 'Corpus Juris', 'CJEU': 'Court of Justice of the European Union', 'CJS': 'Corpus Juris Secundum', 'CLSA': 'Canon Law Society of America', 'Co. Lit. or Co. Litt.': 'Coke on Littleton', 'Co.': 'company', \"Comm'n\": 'commission', \"Comm'r\": 'commissioner', 'Cong. Rec.': 'Congressional Record', 'Cor.': 'corner', 'Corp.': 'Corporation', 'CRS': 'Congressional Research Service', 'Ct. Cl.': 'the United States Court of Federal Claims Reporter', 'C': 'Contract', 'Δ': 'Defendant', 'D': 'Defendant', 'DAC': 'Days After Contract', 'd/b/a': 'doing business as', 'Decr.': 'Decretum', \"Dep't\": 'department', 'DLR': 'Dominion Law Reports', 'DoCRA': 'Duty of Care Risk Analysis Standard', 'DWOP': 'dismissal for want of prosecution', 'ECHR': 'European Convention on Human Rights', 'ECtHR': 'European Court of Human Rights', 'EComHR': 'European Commission of Human Rights', 'ECJ': 'European Court of Justice', 'EGC': 'European General Court', 'ELR': 'European Law Reporter', 'ER': 'Employer', 'EE': 'Employee', 'Esq.': 'Esquire', 'et als.': 'and others', 'et seq.': 'and following', 'ex. p.': 'Ex parte', 'F.': 'Federal Reporter', 'F.2d': 'Federal Reporter, 2nd Series', 'F.3d': 'Federal Reporter, 3rd Series', \"F.App'x\": 'Federal Appendix', 'F.Cas.': 'Federal Cases 1789:1880', 'Fed. Reg.': 'Federal Register', 'FR': 'Federal Register', 'Fed. R. Bankr. P.': 'Federal Rules of Bankruptcy Procedure', 'Fed. R. Civ. P.': 'Federal Rules of Civil Procedure', 'FRCP': 'Federal Rules of Civil Procedure', 'Fed. R. Crim. P.': 'Federal Rules of Criminal Procedure', 'Fed. R. Evid.': 'Federal Rules of Evidence', 'FRE': 'Federal Rules of Evidence', 'f/k/a': 'formerly known as', 'F. Supp.': 'Federal Supplement', 'F. Supp. 2d': 'Federal Supplement, 2nd Series', 'f/t/a': 'failed to appear', 'GAL': 'Guardian ad litem', 'GATT': 'General Agreement on Tariffs and Trade', 'GC': 'General Counsel', 'GVR': 'Grant, Vacate, and Remand', 'NGO': 'Non Government Organization', \"Gov't\": 'government', 'HC': 'Hypothetical Client or High Court', 'HDC': 'Holder in due course', 'ICJ': 'International Court of Justice', 'Id.': 'the same', 'I.L.M.': 'International Legal Materials', 'ILJ': 'Industrial Law Journal', 'IRB': 'Internal Revenue Bulletin', 'ILRM': 'Irish Law Reports Monthly', 'IR': 'Irish Law Reports', 'IRC': 'Internal Revenue Code', 'ISLN': 'International Standard Lawyer Number', 'Inc.': 'Incorporated', 'Ins.': 'insurance', 'Instr.': 'Instructio, a kind of decree', 'In re': 'in the matter of', \"Int'l\": 'international', 'J.': 'Judge', 'JA': 'Appellate judge', 'JD': 'Juris Doctor', 'JDX': 'jurisdiction', 'JCD': 'Juris Canonici Doctor, Doctor of Canon Law', 'JCL': 'Juris Canonici Licentiatus, Licentiate of Canon Law', 'JOL': 'Judgments Online', 'JJ': 'Judges', 'JMOL': 'Judgment as a matter of law', 'JNOV': 'Judgment notwithstanding verdict', 'Jx': 'Jurisdiction', 'JU': 'disposed of by Judge', 'JUST.': 'Justice', 'KC': \"King's Counsel\", 'L/C': 'Letter of credit', 'L&T': 'Landlord & Tenant', 'L.Ed': 'Lawyers Edition', 'L.Ed.2d': 'Lawyer’s Edition 2d Series', 'LJ': 'Postnominals of a Lord or Lady Justice of Appeal', 'LJJ': 'Postnominals of Lords or Ladies Justice of Appeal', 'LL.B.': 'Bachelor of Laws', 'LLC': 'Limited liability company', 'LL.D.': 'Doctor of Law', 'LL.M.': 'Master of Laws', 'LP': 'Limited partnership', 'LLP': 'Limited liability partnership', 'LLLP': 'Limited liability limited partnership', 'LOI': 'Letter of Intent', 'Ltd.': 'Limited', 'MIL': 'Motion in limine', 'MLR': 'Modern Law Review', 'MOU': 'Memorandum of Understanding', 'M.P.': 'motu proprio', 'MPC': 'Model Penal Code', 'MR': 'Postnominals of the Master of the Rolls', 'MSJ': 'Motion for summary judgment', \"Nat'l\": 'national', 'NDA': 'Non-Disclosure Agreement', 'n/k/a': 'Now Known As', 'N.E.': 'Northeastern Reporter', 'N.E.2d': 'North Eastern Reporter, 2nd Series', 'No.': 'Number', 'N.W.': 'Northwestern Reporter', 'N.W.2d': 'North Western Reporter, 2nd Series', 'oao': 'on the application of', \"Opp'n\": 'opposition', 'O.R.C.': 'Ohio Revised Code', 'Org.': 'organization', 'Ors': 'Others', '¶ (Pilcrow)': 'Paragraph', 'Π (Greek letter Pi)': 'Plaintiff', 'P.': 'Pacific Reporter', 'P.2d': 'Pacific Reporter, 2nd Series', 'P.3d': 'Pacific Reporter, 3rd Series', 'p.': 'Page', 'pp.': 'Pages', 'PA': 'Professional association', 'PC': 'Professional corporation', 'PH': 'Prentice Hall Weekly Legal Service', 'PL': 'Public Law', 'PLLC': 'Professional limited liability company', 'POA': 'power of attorney', 'Prae.': 'Praenotanda', 'Pty': 'proprietary company', 'Pub.L.': 'Public Law', 'QC': \"Queen's Counsel\", 'QDRO': 'Qualified Domestic Relations Order', 'R': 'Rex or Regina', 'RCW': 'Revised Code of Washington', 'R.E.': 'Real Estate', 'R/E': 'Real Estate', 'Re': 'In re', \"Reh'g\": 'Rehearing', 'Relv.': 'Relevant', 'Rescr.': 'Rescriptum', 'Resp.': 'Responsum', \"Resp't\": 'Respondent', \"Rev'd\": 'reversed', 'Rev. Proc.': 'Revenue Procedure', 'Rev. Rul.': 'Revenue Ruling', 'RJ': 'Restorative justice', 'R.O.I': 'Release of Information', 'Canon law': 'Regulæ Juris of Boniface VIII', 'Common law': 'Recurring Judgement', 'R.I.A.A.': 'Reports of International Arbitral Awards', '§': 'Section', 's.': 'Section','§§': 'Sections', 'ss.':'Sections', 'SA': 'South African Law Reports', 'SACR': 'South African Criminal Law Reports', 'SALLR': 'South African Labour Law Reports', 'SC': 'Senior Counsel', 'sc.': 'scilicet', 'sd': 'said', 'S.C.R.': 'Supreme Court Reports', 'SCR': 'Supreme Court Reports', 'S. Ct.': 'Supreme Court Reporter', 'S.E.': 'Southeastern Reporter', 'S.E.2d': 'South Eastern Reporter, 2nd Series', 'SCOTUS': 'Supreme Court of the United States', 'SI': 'Statutory instruments', 'S/J': 'Summary judgment', 'SMJ': 'Subject-matter jurisdiction', 'So.': 'Southern Reporter', 'So. 2d': 'Southern Reporter, 2nd Series', 'SOL': 'Statute of Limitations', 'SOR': 'Statutory Orders and Regulations', 'S.R.R.': 'Tribunal of the Roman Rota', 'SRRDec': 'Sacræ Rotæ Romanæ Decisiones[5]', 'Stat.': 'U.S. Statutes-at-Large', 'S.W.': 'Southwestern Reporter', 'S.W.2d': 'South Western Reporter, 2nd Series', 'S.W.3d': 'South Western Reporter, 3rd Series', 'T.C.': 'United States Tax Court Reports', 'T.D.': 'Treasury Decision', '™': 'Trademark', 'TM': 'Trademark', 'UD': 'Unnatural Death', 'UCC': 'Uniform Commercial Code', 'UCMJ': 'Uniform Code of Military Justice', 'UKPC': 'Privy Council of the United Kingdom', 'UKSC': 'Supreme Court of the United Kingdom', 'UPC': 'Uniform Probate Code', 'USC': 'United States Code', 'USCA': 'United States Code Annotated', 'USCCAN': 'United States Code Congressional and Administrative News', 'USCS': 'United States Code Service', 'UST': 'United States Treaties and Other International Agreements', 'v.': 'versus', 'V-C': 'Postnominals of the Vice-Chancellor of the High Court', 'VC': 'Postnominals of the Vice-Chancellor of the High Court', 'VOP': 'Violation of probation', 'WAC': 'Washington Administrative Code', 'WTO': 'World Trade Organization', 'W. Va. Code': 'West Virginia Code', 'WOP': 'without prejudice', 'w/o/p': 'without prejudice', 'XFD': 'Examination for Discovery', 'XN': 'Examination in Chief', 'XXN': 'Cross-examination', 'B.U. L.': 'Boston University Law Review', 'Buff. L.': 'Buffalo Law Review', 'Cal. L.': 'California Law Review', 'Case W. Res. L.': 'Case Western Reserve Law Review', 'Colum. L.': 'Columbia Law Review', 'Cornell L.': 'Cornell Law Review', 'Duke L.J.': 'Duke Law Journal', 'Fordham L.': 'Fordham Law Review', 'Geo. L.J.': 'Georgetown Law Journal', 'Geo. Wash. L.': 'George Washington Law Review', 'Harv. L.': 'Harvard Law Review', 'How. L.J.': 'Howard Law Journal', 'Mich. L.': 'Michigan Law Review', 'Minn. L.': 'Minnesota Law Review', 'N.Y.U. L.': 'New York University Law Review', 'Ohio St. L.J.': 'Ohio State Law Journal', 'Rutgers L.': 'Rutgers Law Review', 'Seton Hall L.': 'Seton Hall Law Review', 'Stan. L.': 'Stanford Law Review', 'Sup. Ct.': 'Supreme Court Review', 'Temp. L.': 'Temple Law Review', 'Tex. L.': 'Texas Law Review', 'UCLA L.': 'UCLA Law Review', 'U. Chi. L.': 'University of Chicago Law Review', 'U. Pa. L.': 'University of Pennsylvania Law Review', 'Vand. L.': 'Vanderbilt Law Review', 'Va. L.': 'Virginia Law Review', 'Wash. U. L.Q.': 'Washington University Law Quarterly', 'Wis. L.': 'Wisconsin Law Review', 'Yale L.J.': 'Yale Law Journal', 'A.A.': 'Atlantic Reporter', 'ADA': 'Americans with Disabilities Act', 'ALR': 'American Law Reports', 'ALR2d': 'American Law Reports, 2d Series', 'AmJur': 'American Jurisprudence', 'AmJur2d': 'American Jurisprudence 2d', 'AmJurPOF': 'American Jurisprudence Proof of Facts', 'B.R.': 'or B.R.W. Bankruptcy Reporter', 'Cal.': 'Rptr. California Reporter (West’s)', 'CrL': 'Criminal Law Reporter', 'FamLRep.': 'Family Law Reporter (BNA)', 'Fed.Cl.': 'Federal Claims Reporter', 'Fed.R.Evid.Serv.': 'Federal Rules of Evidence Service', 'FEP': 'Cases Fair Employment Practice Cases (BNA)', 'F.R.': 'Federal Register', 'FRD': 'Federal Rules Decisions', 'F.S.': 'or F.Supp. Federal Supplement', 'H.': 'or Haw. Hawai‘i Reports', 'H.A.': 'or Haw.App. Hawai‘i Appellate Reports', 'HBJ': 'Hawai‘i Bar Journal', 'HBN': 'Hawai‘i Bar News', 'HLR': 'Hawai‘i Legal Reporter', 'HRS': 'Hawai‘i Revised Statutes', 'HRSA': 'Hawai‘i Revised Statutes Annotated', 'Last': 'Updated: January', 'Abbreviation': 'Title Class No. or Call No. Range', 'I': '& N Dec. Immigration and Nationality Decisions', 'LRRM': 'Labor Relations Reference Manual (BNA)', 'Moore’s': 'Moore’s Federal Practice', 'N.Y.S.': 'New York Supplement', 'P.L.': 'Public Laws (in U.S. Statutes-at-Large)', 'PUR': 'Public Utilities Reports', 'S.Ct.': 'Supreme Court Reporter', 'T.I.A.S.': 'United States Treaties and other International Act Series', 'UHLR': 'or U.Hawai‘i L.Rev. University of Hawai‘i Law Review', 'ULA': 'Uniform Laws Annotated', 'U.S.C.': 'United States Code', 'U.S.C.A.': 'United States Code Annotated', 'USLW': 'United States Law Week', 'U.S.D.C.': 'Haw Reports of the U.S. District Court of Hawai‘i', 'U.S.T.': 'United States Treaties and Other International Agreements', 'USTC': 'U.S. Tax Cases (CCH)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [1:24:50<00:00, 11.79 examples/s]   \n",
      "Map: 100%|██████████| 10000/10000 [12:31<00:00, 13.32 examples/s] \n",
      "Map: 100%|██████████| 10000/10000 [12:23<00:00, 13.46 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# file: legal_classification_pipeline.py\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# Step 1: Abbreviation Dictionary (Handcrafted Example)\n",
    "LEGAL_ABBREVIATIONS = {\n",
    "    \"U.S.C.\": \"United States Code\",\n",
    "    \"id.\": \"same as previous citation\",\n",
    "    \"Pet’r\": \"Petitioner\",\n",
    "    \"Ltd.\": \"Limited\"\n",
    "    # Add more abbreviations as needed\n",
    "}\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for abbr, full in LEGAL_ABBREVIATIONS.items():\n",
    "        text = re.sub(rf\"\\\\b{re.escape(abbr)}\\\\b\", full, text)\n",
    "    return text\n",
    "\n",
    "# Step 2: Metadata Removal (Regex based)\n",
    "def remove_metadata(text):\n",
    "    text = re.sub(r\"^.*?Decided on.*?$\", \"\", text, flags=re.MULTILINE)  # Example: remove dates\n",
    "    text = re.sub(r\"^.*?Case No.*?$\", \"\", text, flags=re.MULTILINE)     # Example: remove case numbers\n",
    "    return text\n",
    "\n",
    "# Step 3: Paragraph Segmentation\n",
    "def segment_paragraphs(text):\n",
    "    return [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "\n",
    "# Step 4: Paragraph Filtering using STF-IDF\n",
    "def semantic_filtering(paragraphs, model, tokenizer, top_k=5):\n",
    "    para_texts = [p for p in paragraphs if p.strip()]\n",
    "    \n",
    "    # 🚨 ADD THIS CHECK\n",
    "    if len(para_texts) == 0:\n",
    "        return []\n",
    "    \n",
    "    encoded_paras = tokenizer(para_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        para_embeds = model(**encoded_paras).last_hidden_state[:, 0, :]\n",
    "\n",
    "    doc_embedding = para_embeds.mean(dim=0, keepdim=True)\n",
    "    similarities = cosine_similarity(para_embeds, doc_embedding)\n",
    "    top_k_indices = similarities[:, 0].argsort()[-top_k:][::-1]\n",
    "    return [para_texts[i] for i in top_k_indices]\n",
    "\n",
    "# Step 5: Map-style Preprocessing Function\n",
    "def create_map_function(model, tokenizer, top_k=5):\n",
    "    def preprocess_example(example):\n",
    "        text = expand_abbreviations(example['text'])\n",
    "        text = remove_metadata(text)\n",
    "        paragraphs = segment_paragraphs(text)\n",
    "        selected = semantic_filtering(paragraphs, model, tokenizer, top_k=top_k)\n",
    "        return {\"text\": \"\\n\".join(selected)}\n",
    "    return preprocess_example\n",
    "\n",
    "# Step 6: Load LexGLUE Dataset and Apply Preprocessing\n",
    "def preprocess_lexglue(dataset_name, model_name=\"nlpaueb/legal-bert-base-uncased\"):\n",
    "    full_dataset = load_dataset(\"lex_glue\", dataset_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    map_fn = create_map_function(bert_model, tokenizer)\n",
    "    processed_dataset = full_dataset.map(map_fn, batched=False)\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "# Usage example:\n",
    "processed_dataset = preprocess_lexglue(\"ledgar\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 60/60 [00:00<00:00, 396.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 424.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 326.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/victorambrose11/ledgar_preprocessed_test/commit/4a724a1d5bac0f38f62e4325fa1a4bc8e3e4f6c6', commit_message='Upload dataset', commit_description='', oid='4a724a1d5bac0f38f62e4325fa1a4bc8e3e4f6c6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/victorambrose11/ledgar_preprocessed_test', endpoint='https://huggingface.co', repo_type='dataset', repo_id='victorambrose11/ledgar_preprocessed_test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processed_dataset.push_to_hub(\"victorambrose11/ledgar_preprocessed_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "\n",
    "original_ledgar = datasets.load_dataset(\"coastalcph/lex_glue\", \"ledgar\")\n",
    "processed_dataset=datasets.load_dataset('victorambrose11/ledgar_preprocessed_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 21:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.446200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:12<00:00, 4729.01 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4924.76 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4843.20 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 21:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:21<00:00, 2809.93 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 2611.19 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:03<00:00, 2879.15 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 21:04, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:16<00:00, 3663.43 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3766.57 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3341.77 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 21:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:13<00:00, 4485.05 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5025.66 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4943.32 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 11:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000/60000 [00:16<00:00, 3556.49 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3366.15 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 4069.79 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3396347/1524936703.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1650' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1650/3752 05:15 < 06:42, 5.22 it/s, Epoch 1.76/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/srmist49/miniconda3/envs/victorml/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     77\u001b[39m         results.append({\n\u001b[32m     78\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m'\u001b[39m: dataset_name,\n\u001b[32m     79\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: model_alias,\n\u001b[32m     80\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mOriginal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     81\u001b[39m             **metrics_original\n\u001b[32m     82\u001b[39m         })\n\u001b[32m     84\u001b[39m         \u001b[38;5;66;03m# Train on preprocessed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         metrics_preprocessed = train_model(model_checkpoint, preprocessed, num_labels, output_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_alias\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_preprocessed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     86\u001b[39m         results.append({\n\u001b[32m     87\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m'\u001b[39m: dataset_name,\n\u001b[32m     88\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: model_alias,\n\u001b[32m     89\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPreprocessed\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     90\u001b[39m             **metrics_preprocessed\n\u001b[32m     91\u001b[39m         })\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Convert to DataFrame and print cleanly\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_name, dataset_dict, num_labels, output_dir)\u001b[39m\n\u001b[32m     30\u001b[39m args = TrainingArguments(\n\u001b[32m     31\u001b[39m     output_dir=output_dir,\n\u001b[32m     32\u001b[39m     do_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     save_total_limit=\u001b[32m1\u001b[39m\n\u001b[32m     41\u001b[39m )\n\u001b[32m     45\u001b[39m trainer = Trainer(\n\u001b[32m     46\u001b[39m     model=model,\n\u001b[32m     47\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m trainer.train()\n\u001b[32m     56\u001b[39m metrics = trainer.evaluate(tokenized_datasets[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2246\u001b[39m         args=args,\n\u001b[32m   2247\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2248\u001b[39m         trial=trial,\n\u001b[32m   2249\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28mself\u001b[39m.get_batch_samples(epoch_iterator, num_batches, args.device)\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/accelerate/data_loader.py:575\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m         current_batch = send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28mself\u001b[39m._non_blocking)\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    577\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/accelerate/utils/operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device, non_blocking=non_blocking)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/victorml/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:821\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    818\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    820\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m         k: v.to(device=device, non_blocking=non_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    823\u001b[39m     }\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    825\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    micro_f1 = f1_score(labels, preds, average='micro')\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1\n",
    "    }\n",
    "\n",
    "def train_model(model_name, dataset_dict, num_labels, output_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate(tokenized_datasets['test'])\n",
    "    return metrics\n",
    "\n",
    "# Datasets and Models setup\n",
    "datasets = {\n",
    "    'LEDGAR': (original_ledgar, processed_dataset, 100)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'RobertaBase': 'roberta-base',\n",
    "    'LegalBERT': 'nlpaueb/legal-bert-base-uncased',\n",
    "    'DistilBERT': 'distilbert-base-uncased'\n",
    "}\n",
    "\n",
    "# Results collection\n",
    "results = []\n",
    "\n",
    "for dataset_name, (original, preprocessed, num_labels) in datasets.items():\n",
    "    for model_alias, model_checkpoint in models.items():\n",
    "        # Train on original\n",
    "        metrics_original = train_model(model_checkpoint, original, num_labels, output_dir=f'./{model_alias}_{dataset_name}_original')\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_alias,\n",
    "            'Data': 'Original',\n",
    "            **metrics_original\n",
    "        })\n",
    "\n",
    "        # Train on preprocessed\n",
    "        metrics_preprocessed = train_model(model_checkpoint, preprocessed, num_labels, output_dir=f'./{model_alias}_{dataset_name}_preprocessed')\n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_alias,\n",
    "            'Data': 'Preprocessed',\n",
    "            **metrics_preprocessed\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and print cleanly\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "victorml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
