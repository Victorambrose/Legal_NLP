{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\"\"\" Finetuning models on SCOTUS (e.g. Bert, RoBERTa, LEGAL-BERT).\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from models.hierbert import HierarchicalBert\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "from models.deberta import DebertaForSequenceClassification\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.9.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                    \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_segments: Optional[int] = field(\n",
    "        default=64,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum number of segments (paragraphs) to be considered. Sequences longer \"\n",
    "                    \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_seg_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                    \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },=\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    server_ip: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n",
    "    server_port: Optional[str] = field(default=None, metadata={\"help\": \"For distant debugging.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    hierarchical: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to use a hierarchical variant or not\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    do_lower_case: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if data_args.server_ip and data_args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Fix boolean parameter\n",
    "    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:\n",
    "        model_args.do_lower_case = False\n",
    "    else:\n",
    "        model_args.do_lower_case = True\n",
    "\n",
    "    if model_args.hierarchical == 'False' or not model_args.hierarchical:\n",
    "        model_args.hierarchical = False\n",
    "    else:\n",
    "        model_args.hierarchical = True\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    # Downloading and loading eurlex dataset from the hub.\n",
    "    if training_args.do_train:\n",
    "        train_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"train\", cache_dir=model_args.cache_dir)\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        eval_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"validation\", cache_dir=model_args.cache_dir)\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        predict_dataset = load_dataset(\"lex_glue\", \"scotus\", split=\"test\", cache_dir=model_args.cache_dir)\n",
    "\n",
    "    # Labels\n",
    "    label_list = list(range(14))\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=\"scotus\",\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        do_lower_case=model_args.do_lower_case,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    if config.model_type == 'deberta' and model_args.hierarchical:\n",
    "        model = DebertaForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    if model_args.hierarchical:\n",
    "        # Hack the classifier encoder to use hierarchical BERT\n",
    "        if config.model_type in ['bert', 'deberta']:\n",
    "            if config.model_type == 'bert':\n",
    "                segment_encoder = model.bert\n",
    "            else:\n",
    "                segment_encoder = model.deberta\n",
    "            model_encoder = HierarchicalBert(encoder=segment_encoder,\n",
    "                                             max_segments=data_args.max_segments,\n",
    "                                             max_segment_length=data_args.max_seg_length)\n",
    "            if config.model_type == 'bert':\n",
    "                model.bert = model_encoder\n",
    "            elif config.model_type == 'deberta':\n",
    "                model.deberta = model_encoder\n",
    "            else:\n",
    "                raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n",
    "        elif config.model_type == 'roberta':\n",
    "            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,\n",
    "                                             max_segment_length=data_args.max_seg_length)\n",
    "            model.roberta = model_encoder\n",
    "            # Build a new classification layer, as well\n",
    "            dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights\n",
    "            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)\n",
    "            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)\n",
    "            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights\n",
    "            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)\n",
    "        elif config.model_type in ['longformer', 'big_bird']:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{config.model_type} is no supported yet!\")\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    # Padding strategy\n",
    "    if data_args.pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "    else:\n",
    "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "        padding = False\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        if model_args.hierarchical:\n",
    "            case_template = [[0] * data_args.max_seq_length]\n",
    "            if config.model_type == 'roberta':\n",
    "                batch = {'input_ids': [], 'attention_mask': []}\n",
    "                for doc in examples['text']:\n",
    "                    doc = re.split('\\n{2,}', doc)\n",
    "                    doc_encodings = tokenizer(doc[:data_args.max_segments], padding=padding,\n",
    "                                              max_length=data_args.max_seg_length, truncation=True)\n",
    "                    batch['input_ids'].append(doc_encodings['input_ids'] + case_template * (\n",
    "                            data_args.max_segments - len(doc_encodings['input_ids'])))\n",
    "                    batch['attention_mask'].append(doc_encodings['attention_mask'] + case_template * (\n",
    "                            data_args.max_segments - len(doc_encodings['attention_mask'])))\n",
    "            else:\n",
    "                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}\n",
    "                for doc in examples['text']:\n",
    "                    doc = re.split('\\n{2,}', doc)\n",
    "                    doc_encodings = tokenizer(doc[:data_args.max_segments], padding=padding,\n",
    "                                              max_length=data_args.max_seg_length, truncation=True)\n",
    "                    batch['input_ids'].append(doc_encodings['input_ids'] + case_template * (\n",
    "                                data_args.max_segments - len(doc_encodings['input_ids'])))\n",
    "                    batch['attention_mask'].append(doc_encodings['attention_mask'] + case_template * (\n",
    "                                data_args.max_segments - len(doc_encodings['attention_mask'])))\n",
    "                    batch['token_type_ids'].append(doc_encodings['token_type_ids'] + case_template * (\n",
    "                                data_args.max_segments - len(doc_encodings['token_type_ids'])))\n",
    "        elif config.model_type in ['longformer', 'big_bird']:\n",
    "            cases = []\n",
    "            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \\\n",
    "                else config.max_position_embeddings\n",
    "            for doc in examples['text']:\n",
    "                doc = re.split('\\n{2,}', doc)\n",
    "                cases.append(f' {tokenizer.sep_token} '.join([' '.join(paragraph.split()[:data_args.max_seg_length])\n",
    "                                                              for paragraph in doc[:data_args.max_segments]]))\n",
    "            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)\n",
    "            if config.model_type == 'longformer':\n",
    "                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)\n",
    "                # global attention on cls token\n",
    "                global_attention_mask[:, 0] = 1\n",
    "                batch['global_attention_mask'] = list(global_attention_mask)\n",
    "        else:\n",
    "            batch = tokenizer(examples['text'], padding=padding, max_length=512, truncation=True)\n",
    "\n",
    "        batch[\"label\"] = [label_list.index(labels) for labels in examples[\"label\"]]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if data_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "            train_dataset = train_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "        # Log a few random samples from the training set:\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "            eval_dataset = eval_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n",
    "        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "            predict_dataset = predict_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                load_from_cache_file=not data_args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on prediction dataset\",\n",
    "            )\n",
    "\n",
    "    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "    # predictions and label_ids field) and has to return a dictionary string to float.\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        macro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='macro', zero_division=0)\n",
    "        micro_f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro', zero_division=0)\n",
    "        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}\n",
    "\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "    if data_args.pad_to_max_length:\n",
    "        data_collator = default_data_collator\n",
    "    elif training_args.fp16:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "    else:\n",
    "        data_collator = None\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        metrics = train_result.metrics\n",
    "        max_train_samples = (\n",
    "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    # Prediction\n",
    "    if training_args.do_predict:\n",
    "        logger.info(\"*** Predict ***\")\n",
    "        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n",
    "\n",
    "        max_predict_samples = (\n",
    "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "        )\n",
    "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"predict\", metrics)\n",
    "        trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "        output_predict_file = os.path.join(training_args.output_dir, \"test_predictions.csv\")\n",
    "        if trainer.is_world_process_zero():\n",
    "            with open(output_predict_file, \"w\") as writer:\n",
    "                for index, pred_list in enumerate(predictions[0]):\n",
    "                    pred_line = '\\t'.join([f'{pred:.5f}' for pred in pred_list])\n",
    "                    writer.write(f\"{index}\\t{pred_line}\\n\")\n",
    "\n",
    "    # Clean up checkpoints\n",
    "    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]\n",
    "    for checkpoint in checkpoints:\n",
    "        shutil.rmtree(checkpoint)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
